---
title: "Proyecto - Kaggle Titanic"
author: "José Manuel Alvarez García"
date: "Noviembre 06, 2016"
output: pdf_document
---
Para instalar el paquete knit se debe ejecutar lo siguiente:
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
install = function(pkg)
{
  # Si ya está instalado, no lo instala.
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg)
    if (!require(pkg, character.only = TRUE)) stop(paste("load failure:", pkg))
  }
}
```

Inicialmente se debe estar posicionado en el directorio de la tarea:
```{r}
setwd("C:/Users/José Manuel/Documents/ICD/proyecto-kaggle-titanic-josemalvarezg1")
```

# Análisis exploratorio.

Se trabajará con un conjunto de datasets que contiene la base de datos de las personas que abordaron al Titanic, y se tiene como clase principal o variable predictora si una persona sobrevivió a la tragedia o no.

En el mismo también se tiene el siguiente conjunto de atributos (columnas):

1. survival: Representa si una persona sobrevivió a la tragedia o no. Toma como valores 0 si la persona no sobrevivió, y 1 en caso contrario.

2. pclass: Representa la clase pasajera de una persona en el Titanic. Puede tomar los siguientes valores: 1 como 1st (primera clase), 2 como 2nd (segunda clase) y 3 como 3rd (tercera clase). 

3. name: Representa el nombre de una persona. Puede tomar cualquier valor del tipo String.

4. sex: Representa el género de una persona. Puede tomar los siguientes valores: female, male. 

5. age: Representa la edad de una persona. Puede tomar cualquier valor numérico. En caso de que se esté considerando una edad estimada, está será mostrada como xx.5.

6. sibsp: Representa el número de hermanos o cónyugues de una persona. Puede tomar cualquier valor entero.

7. parch: Representa el número de padres o niños (hijos) de una persona. Puede tomar cualquier valor entero.

8. ticket: Representa el número de ticket de una persona. Puede tomar cualquier valor del tipo String.

9. fare: Representa la tarifa pagada (en dólares) por una persona. Puede tomar cualquier valor del tipo flotante.

10. cabin: Representa la cabina en donde estará alojada una persona. Puede tomar cualquier valor del tipo String.

11. embarked: Representa el puerto de embarcación de una persona al abordar el Titanic. Puede tomar los siguientes valores: C si abordó en Cherbourg, Q si abordó en Queenstown y S si abordó en Southampton.

Posteriormente, estas columnas serán renombradas en la tarea de pre-procesamiento.

# Pre-Procesamiento.

Inicialmente se debe leer el dataset de training:
```{r}
train = read.csv(file = "../data/train.csv", header = T)
```
Luego se debe leer el dataset testing:
```{r}
test = read.csv(file = "../data/test.csv", header = T)
```
Se identificarán las columnas de ambos datasets de la siguiente manera:
```{r}
colnames(train) <- c("ID", "Sobrevivio", "Clase", "Nombre", "Sexo", "Edad", "Hermanos/Cónyuges", "Padres/Niños", "Ticket", "Tarifa", "Cabina", "Embarcación")
colnames(test) <- c("ID", "Clase", "Nombre", "Sexo", "Edad", "Hermanos/Cónyuges", "Padres/Niños", "Ticket", "Tarifa", "Cabina", "Embarcación")
```
Se elimina la primera columna (ID) de los datasets. Es considerada innecesaria ya que el ID de cada pasajero es su mismo número de registro (fila) en el dataset.
```{r}
train = train[,-1]
test = test[,-1]
```
Algunos elementos de la columna "Edad" son desconocidos y contienen valores N/A. Por lo que se calcula la edad promedio y es colocada a los N/A.
```{r}
train[,5][is.na(train[, 5])] <- 0
train[,5][train[, 5] == 0] <- ceiling(mean(train[["Edad"]]))
```
Los valores de la columna "Sexo" son cambiados a numéricos. Para que así se identifiquen como Male = 0 y Female = 1.
```{r}
train$Sexo = (train$Sexo=="female")*1
```

Si se desea realizar un análisis exploratorio para estudiar más a fondo el dataset se debe realizar lo siguiente: 
```{r, warning=FALSE}
library(FactoMineR)
```
Se muestran valores de interés del dataset de entrenamiento:
```{r}
head(train)
dim(train)
names(train)
summary(train)
```

Se trabajará sólo con las columnas "Sobrevivió", "Clase", "Sexo", "Edad", "Hermanos/Cónyugues", "Padres/Niños" y "Tarifa" para luego mostrar el PCA:
```{r}
titanicPCA <- subset(train, select = c(1,2,4,5,6,7,9))
```
Se grafica el PCA de la siguiente manera:
```{r}
pca <- PCA(titanicPCA)
```

# K-Medias.
Para aplicar la técnica de clusterización K-Medias, se trabajará sólo con la columna "Edad".
```{r}
trainK = train[,c(5)]
```
Ahora bien, se buscará el K más adecuado para aplicar la técnica. Esto se realizará mediante el Codo de Jambu:
```{r, warning=FALSE}
InerciaIC = rep(0,50)
for (k in 1:50) {
  grupos = kmeans(trainK, k)
  InerciaIC[k] = grupos$tot.withinss
}
```
Si se grafica la Inercia Inter-Clases se puede observar que cambia muy poco a partir de K=4 y K=5.
```{r}
plot(InerciaIC, col = "blue", type = "b")
```

Por lo que calcula K-Medias con K=5 y 100 iteraciones, y se grafica: 
```{r}
clusters <- kmeans(trainK, 5, iter.max = 100) 
plot(trainK, pch = 20)
plot(trainK, col = clusters$cluster)
```

# Clasificación Jerárquica.

Se trabajará el dataset pre-procesado anteriormente como una matriz y luego se calculará la matriz de distancia:
```{r}
datos = as.matrix(trainK)
distancia = dist(datos)
```

Se aplicarán y se graficarán los métodos de clasificación jerárquica.
Para el método Complete:
```{r}
cluster = hclust(distancia, method = "complete")
plot(cluster)
#Se determina la altura requerida con k clusters, cortando el dendograma con k clases:
corteD = cutree(cluster, k = 5)
#Observamos la cantidad de clusters.
unique(corteD)
#Graficamos los clusters.
plot(trainK, col = corteD, main = "COMPLETE")
```

Para el método Single:
```{r}
cluster = hclust(distancia, method = "single")
plot(cluster)
#Se determina la altura requerida con k clusters, cortando el dendograma con k clases:
corteD = cutree(cluster, k = 5)
#Observamos la cantidad de clusters
unique(corteD)
#Graficamos los clusters.
plot(trainK, col = corteD, main = "SINGLE")
```

Para el método Average:
```{r}
cluster = hclust(distancia, method = "average")
plot(cluster)
#Se determina la altura requerida con k clusters, cortando el dendograma con k clases:
corteD = cutree(cluster, k = 5)
#Observamos la cantidad de clusters.
unique(corteD)
#Graficamos los clusters.
plot(trainK, col = corteD, main = "AVERAGE")
```

Para el método Ward:
```{r}
cluster = hclust(distancia, method = "ward.D")
plot(cluster)
#Se determina la altura requerida con k clusters, cortando el dendograma con k clases:
corteD = cutree(cluster, k = 5)
#Observamos la cantidad de clusters.
unique(corteD)
#Graficamos los clusters.
plot(trainK, col = corteD, main = "WARD")
```

Se puede observar que la clasificación por el método Ward es la que más se adapta al clustering del dataset utilizando K-Medias con K=5.

# Reglas de Asociación.
Se deben incluir las bibliotecas "arules" y "arulesViz" de la siguiente manera:
```{r, warning=FALSE, message=FALSE}
library(arules)
library(arulesViz)
```
Para leer el dataset Titanic Raw se debe ejecutar lo siguiente:
```{r, warning=FALSE}
load("../data/titanic.raw.Rdata")
str(titanic.raw)
#Se transforma dataframe en transaccional.
trans <- as(titanic.raw, "transactions")
```

Seguidamente, ya teniendo el dataset como valores transaccionales se generan las reglas de asociación y se muestra un resumen de las mismas:
```{r}
reglas <- apriori(trans)
summary(reglas)
```

Para conocer las 10 transacciones con mayor número de apariciones en el dataset se realiza lo siguiente:
```{r}
itemFrequencyPlot(trans,topN=10,type="absolute")
```

Para ordenar las reglas se tiene lo siguiente:
```{r}
#Se ordenan las reglas por confianza
confianzaAlta <-sort(reglas, by="confidence", decreasing=TRUE)
inspect(head(confianzaAlta))
```
Se observa que las primeras seis reglas tienen una confianza de 1, por lo que estas siempre serán verídicas.

```{r}
#Se ordenan las reglas por soporte
supportAlto <-sort(reglas, by="support", decreasing=TRUE)
inspect(head(supportAlto))
```
Se observa que las primeras seis reglas son las que más frecuentan en el dataset.

```{r}
#Se ordenan las reglas por lift
liftAlto <-sort(reglas, by="lift", decreasing=TRUE)
inspect(head(liftAlto))
```
Se observa que las primeras seis reglas son las más probables en ocurrir.

OJO. Explicar apariciones de estas reglas.

# Árboles de Decisión
Se cargan las bibliotecas para la utilización del método de clasificación de árboles de decisión:
```{r, warning=FALSE}
library("rpart")
library("rpart.plot")
```
Para esta técnica se trabajará sólo con las columnas "Sobrevivio", "Clase", "Sexo", "Edad", "Hermanos/Cónyugues", "Padres/Niños" y "Tarifa".
```{r}
trainT <- subset(train, select = c(1,2,4,5,6,7,9))
```
Se obtiene el árbol de decisión en base a la columna Clase del dataset y se grafica:
```{r}
tree <- rpart(Sobrevivio ~ ., trainT, method = "class")
rpart.plot(tree)
```
Se muestran las predicciones en base al árbol anteriormente generado:
```{r}
p <- predict(tree, trainT, type="class")
table(trainT[,1], p)
```

# Máquinas de Soporte Vectorial
Se debe incluir la biblioteca "e1071" de la siguiente manera:
```{r, warning=FALSE}
library("e1071")
```
Para esta técnica se trabajará sólo con las columnas "Sobrevivio", "Clase", "Sexo", "Edad", "Hermanos/Cónyugues", "Padres/Niños" y "Tarifa" para el conjunto de Training; y las mismas para uno segundo sin incluir la columna "Sobrevivio".
```{r}
trainSVM <- subset(train, select = c(1,2,4,5,6,7,9))
trainSVM2 <- subset(train, select = c(2,4,5,6,7,9))
```

Ahora bien, se debe entrenar el modelo probando diferentes kernels.
Se entrena el modelo con el kernel "sigmoid".
```{r}
svm_model <- svm(Sobrevivio ~ .,kernel="sigmoid",data = trainSVM)
pred <- predict(svm_model,trainSVM2)
pred <- replace(pred, pred < 0.5, 0)
pred <- replace(pred, pred >= 0.5, 1)
```
Se muestra la matriz de confusión.
```{r}
table(pred, trainSVM$Sobrevivio)
```

Se entrena el modelo con el kernel "radial".
```{r}
svm_model <- svm(Sobrevivio ~ .,kernel="radial",data = trainSVM)
pred <- predict(svm_model,trainSVM2)
pred <- replace(pred, pred < 0.5, 0)
pred <- replace(pred, pred >= 0.5, 1)
```
Se muestra la matriz de confusión.
```{r}
table(pred, trainSVM$Sobrevivio)
```

Se entrena el modelo con el kernel "polynomial".
```{r}
svm_model <- svm(Sobrevivio ~ .,kernel="polynomial",data = trainSVM)
pred <- predict(svm_model,trainSVM2)
pred <- replace(pred, pred < 0.5, 0)
pred <- replace(pred, pred >= 0.5, 1)
```
Se muestra la matriz de confusión.
```{r}
#Se muestra la matriz de confusión.
table(pred, trainSVM$Sobrevivio)
```

Se entrena el modelo con el kernel "linear".
```{r}
svm_model <- svm(Sobrevivio ~ .,kernel="linear",data = trainSVM)
pred <- predict(svm_model,trainSVM2)
pred <- replace(pred, pred < 0.5, 0)
pred <- replace(pred, pred >= 0.5, 1)
```
Se muestra la matriz de confusión.
```{r}
table(pred, trainSVM$Sobrevivio)
```

El mejor método fue "radial", por lo que se afina el modelo considerando este kernel.
```{r}
svm_tune <- 0
svm_tune <- tune(svm, train.x=trainSVM, train.y=trainSVM2, kernel="radial", ranges= list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4)))
print(svm_tune)
```
Ahora, se evalúa el modelo luego de afinarlo y conseguir el cost y gamma óptimos. OJO.
```{r}
svm_model_after_tune <- svm(Sobrevivio ~ ., kernel="radial", data = trainSVM, cost=1, gamma=0.5)
summary(svm_model_after_tune)
pred <- predict(svm_model_after_tune,trainSVM2)
pred <- replace(pred, pred < 0.5, 0)
pred <- replace(pred, pred >= 0.5, 1)
```
Se muestra la matriz de confusión.
```{r}
table(pred,trainSVM$Sobrevivio)
```

# Curvas ROC
Se debe incluir la biblioteca "ROCR" de la siguiente manera:
```{r, warning=FALSE}
library("ROCR")
```

